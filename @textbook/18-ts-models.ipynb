{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b46ff9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This lesson is part of the <b>DS Lab core curriculum</b>. For that reason, this notebook can only be used on your WQU virtual machine.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This means:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No downloading this notebook.</li>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No re-sharing of this notebook with friends or colleagues.</li>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No downloading the embedded videos in this notebook.</li>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No re-sharing embedded videos with friends or colleagues.</li>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No adding this notebook to public or private repositories.</li>\n",
    "      <li><span style=\"color: red\">ⓧ</span> No uploading this notebook (or screenshots of it) to other websites, including websites for study resources.</li>\n",
    "    </ul>\n",
    "\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2810ff-7008-47e2-b9af-4d7ae3662784",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+3\"><strong>Time Series: Statistical Models</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ee28e-b2ae-4140-94f3-18320cd6c480",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Autoregression\n",
    "\n",
    "Autoregression (AR) is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. AR works similarly to **autocorrelation**: in both cases, we're taking data from one part of a set and comparing it to another part. An AR model regresses itself. \n",
    "\n",
    "## Cleaning the Data\n",
    "\n",
    "Just like with linear regression, we'll start by bringing in some tools to help us along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cbcf8-4b37-4775-80de-90381cae651c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from arch import arch_model\n",
    "from IPython.display import YouTubeVideo\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dd800-35cd-4fc6-ab6c-8039e7120fac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since we'll be working with the `\"air-quality\"` data again, we need to connect to the server, start our client, and grab the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c05154-4475-4640-b67c-6b3c77533ad9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = client[\"air-quality\"]\n",
    "lagos = db[\"lagos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8156d-5e20-489f-89a9-b279755094bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Just to make sure we're all on the same page, import all those libraries and get your database up and running. Remember that even though all the examples use the Site 3 data from the `lagos` collection, the practice sets should use Site 4 data from the `lagos` collection. Call your database `lagos_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16011813-c00f-43cd-b397-e13ec7ddb0d9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "lagos_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f9986-0e10-4554-952f-89a8eb7ac279",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to get our data into a form we can use to build our model, we're going to need to transform it in several key ways. The first thing we need to do is to get the data we need, and save the results in a DataFrame. Since we're interested in predicting the changes in air quality over time, let's set the DataFrame's index to `\"timestamp\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff06670-cb53-476a-a18f-57a88545208e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "results = lagos.find(\n",
    "    # Note that the `3` refers to Site 3.\n",
    "    {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "    projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    ")\n",
    "df = pd.DataFrame(list(results)).set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cab2b8-4477-4373-9d8d-3fa392d26f36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a list called `results_prac` that pulls data from Site 4 in the `lagos` data, then save it in a DataFrame called `df_prac` with the index `\"timestamp\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8808669-d0d4-4f30-b0c4-ecd8e31cbca0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf7691-0bba-4721-974f-75df00f1286d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Localizing the Timezone\n",
    "\n",
    "Because MongoDB stores all timestamps in `UTC`, we need to figure out a way to localize it. Having timestamps in UTC might be useful if we were trying to predict some kind of global trend, but since we're only interested in what's happening with the air in Lagos, we need to change the data from UTC to `Africa/Lagos`. Happily, pandas has a pair of tools to help us out: [`tz_localize`](https://pandas.pydata.org/docs/reference/api/pandas.Series.tz_localize.html) and [`tz_convert`](https://pandas.pydata.org/docs/reference/api/pandas.Series.tz_convert.html). We use those methods to transform our data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04d148-e8e3-4d9a-a9fc-10e5316400b8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec7e24-1d2a-4c5e-9a2a-79a4b2cebf07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Resampling Data\n",
    "\n",
    "The most important kind of data in our time-series model is the data that deals with time. Our `\"timestamp\"` data tells us when each reading was taken, but in order to create a good predictive model, we need the readings to happen at regular intervals. Our data doesn't do that, so we need to figure out a way to change it so that it does. The [`resample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html) method does that for us. \n",
    "\n",
    "Let's resample our data to create 1-hour reading intervals by aggregating using the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a11928-52f4-448d-beef-0c78bd92e92e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# `\"1H\"` represents our one-hour window\n",
    "df = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\").to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019220d8-406d-452c-ac64-938d6db60884",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice the second half of the code:\n",
    "\n",
    "```python\n",
    "fillna(method=\"ffill\").to_frame()\n",
    "```\n",
    "\n",
    "That tells the model to **forward-fill** any empty cells with **imputed** data. Forward-filling means that the model should start imputing data based on the closest cell that actually has data in it. This helps to keep the imputed data in line with the rest of the dataset. \n",
    "\n",
    "## Adding a Lag\n",
    "\n",
    "We've spent some time elsewhere thinking about how two sets of data — apartment price and location, for example — compare to *each other*, but we haven't had any reason to consider how a dataset might compare to *itself*. If we're predicting the future, we want to know how good our prediction will be, so it might be useful to build some of that accountability into our model. To do that, we need to add a **lag**.\n",
    "\n",
    "Lagging data means that we're adding a delay. In this case, we're going to allow the model to test itself out by comparing its predictions with what actually happened an hour before. If the prediction and the reality are close, then it's a good model; if they aren't, then the model isn't a very good one.\n",
    "\n",
    "So, let's add a one-hour lag to our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ba14a-46fd-4763-ab8b-145d9c44e70d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# In `shift(1), the `1` is the lagged interval.\n",
    "df[\"P2.L1\"] = df[\"P2\"].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74639-fe8a-47a3-ac2c-49c9cef99715",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally, let's drop our null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58143a-83d9-4225-a091-29c583ea0f11",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "y = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dd309-0abe-4213-a329-d05852d1cf15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Clean the Site 2 data from `lagos`, and save it as a Series called `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf4ae9-fb22-4659-895b-e27be5eb714a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df_prac.index = ...\n",
    "df_prac = ...\n",
    "df_prac[\"P2.L1\"] = ...\n",
    "\n",
    "\n",
    "y_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96cc3e-8f61-4deb-ab7e-7251ec7c8aed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350e59a-48f9-4ac1-b189-2bcaf8dbe1bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Time Series Line Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013efa0c-63d1-47f1-8fcd-4e143a27fbee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Example of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5c73e-26d7-4c4c-b0b4-aec48ab49a8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Creating an ACF Plot\n",
    "\n",
    "Let's make an ACF plot using our `y` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f174d5-c693-4521-9984-608d3e15dc29",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "# This is where to include your Series\n",
    "\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c54acc-93e1-4c33-adc5-789d4141b954",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Each of the dots on our plot represents a correlation coefficient. The first data point in the top left of the graph tells us that at time-step 0, the correlation coefficient was 1, meaning that there was a perfect correlation. That makes sense, because you can't lag from time-step 0, so the coefficient can't be anything other than 1. But, starting at hour 1, the coefficient drops precipitously, and we see our autocorrelation coefficients slowly decay over time. As our lag recedes further into the past, the correlations break down; a prediction you made five hours ago about what's happening right now is going to be a lot more reliable than a prediction you made 96 hours ago.\n",
    "\n",
    "The light blue shape across the bottom of the graph represents the **confidence interval**, or the extent to which we can be sure that our estimated correlations reflect the correlations that exist in reality. By default, this is set to 95%. Data points which fall either above or below the shape are likely not due to chance, and those which fall inside the shape are likely due to chance. It looks like all our data is the result of some kind of effect, so we're good to go.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make an ACF plot called `fig2` using your `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e104d-29af-4b09-84f8-347371ca9ebd",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig2, ax = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735a570-a592-48c8-ad48-be4a66e0f68f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Creating a PACF Plot\n",
    "\n",
    "Let's make a PACF plot using our `y` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fd648-c207-4d6b-a7f8-6857845ec1ef",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f134b-593c-4f92-b9e3-321cbf5dc5b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Aha! This looks very different. There are two things to notice here:\n",
    "\n",
    "First, we now have lots of data points that we can be relatively certain aren't due to chance, but we also have lots of data points inside the blue shape at the bottom, indicating that some of our data points are indeed due to chance. That's not necessarily a problem, but it's something useful to keep in mind.\n",
    "\n",
    "Second, recognize that even though the amplitude of the points on our graph has been significantly reduced, the trend has remained essentially the same: Strong positive correlations at the beginning, with the effect decaying over time. We would expect to see that, because the farther out into the future our predictions go, the less accurate they become. \n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make an PACF plot using your `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b819645-5e75-4e1b-93f6-2da7e6c956a2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig2, ax = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf9f5d-9fb0-4bca-8ac9-c24903932354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Working with Rolling Windows\n",
    "\n",
    "**Rolling window** is an important concept for time series analysis. We first define a window size, like 7 days, three months, etc. Then we calculate some statistics taking data from each window sequentially throughout the time series. For example, if I want to calculate a three-month rolling sum with the time series data below:\n",
    "\n",
    "<table>\n",
    "    \n",
    "<tr>\n",
    "<th style=\"text-align: left\">Month</th>\n",
    "<th style=\"text-align: left\">sales</th>\n",
    "\n",
    "</tr>    \n",
    "<tr>\n",
    "<th style=\"text-align: left\">2022-01</th>\n",
    "<th style=\"text-align: left\">10</th>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-02</td>\n",
    "<td style=\"text-align: left\">20</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-03</td>\n",
    "<td style=\"text-align: left\">25</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th style=\"text-align: left\">2022-04</th>\n",
    "<th style=\"text-align: left\">15</th>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-05</td>\n",
    "<td style=\"text-align: left\">20</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-06</td>\n",
    "<td style=\"text-align: left\">30</td>\n",
    "</tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "The three-month rolling sum would be\n",
    "\n",
    "<table>\n",
    "    \n",
    "<tr>\n",
    "<th style=\"text-align: left\">Rolling Months</th>\n",
    "<th style=\"text-align: left\">Rolling sum sales</th>\n",
    "\n",
    "</tr>        \n",
    "<tr>\n",
    "<th style=\"text-align: left\">2022-01,02,03</th>\n",
    "<th style=\"text-align: left\">55</th>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-02,03,04</td>\n",
    "<td style=\"text-align: left\">60</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">2022-03,04,05</td>\n",
    "<td style=\"text-align: left\">60</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th style=\"text-align: left\">2022-04,05,06</th>\n",
    "<th style=\"text-align: left\">65</th>\n",
    "\n",
    "</tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d9d65-729f-40bc-9d4e-3caf896b78b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Rolling window statistics are very helpful in smoothing noisy data when making time series predictions. Let's see it with an example. Since we're interested in making predictions about the air quality in Lagos, it would be helpful to understand the rolling average for the PM 2.5 readings with a line plot. To keep things manageable, we'll set our window-size to one week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb472e-a4b2-4605-8959-117da242abd7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# `168` is the number of hours in a week.\n",
    "df[\"P2\"].rolling(168).mean().plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8dbd9-810d-4d54-9c76-1307f68f720a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Even though there are lots of peaks and valleys here, we're starting to see an emerging trend.\n",
    "\n",
    "We can make the same graph using pandas, like this:\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a line plot that shows the weekly rolling average of the `P2` values in the `Site 2` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016358cb-3508-423c-823b-e48c8150c561",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973c0f71-69da-4009-b0c8-df26cdcbab73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Besides rolling sum and rolling average, rolling window statistics can be applied to a lot of other statistics depends on the problem you are facing. In the example below, when we use **GARCH** model to analyze stock prices, we can use rolling window to calculate standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6e88-8590-41e9-87a9-ad9ed4e1d4fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Splitting the Data in pandas\n",
    "\n",
    "The last thing to do in our data exploration is to split our data into training and test sets. For linear regression, we used an 80/20 split, where we used 80% of the data was our training set, and 20% of it was our test set. This time, we're going to expand the test set to 95%, and decrease the test set to %5 to bring it into line with `statsmodels` default confidence interval. This is important, because we'll need to use as much training data as we can if our model is going to accurately predict what's going to come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9841f7-7cde-430d-9629-fa1ee94b0024",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cutoff_test1 = int(len(y) * 0.95)\n",
    "\n",
    "y_train = y.iloc[:cutoff_test1]\n",
    "y_test = y.iloc[cutoff_test1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4552f1-753e-49a3-8d6e-42157bfa1c37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a cutoff called `cutoff_test2`, split the `y_prac`Series into training and test sets, making sure to set the cutoff to 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97b3e6-2ac9-4ecf-b074-195ed04612f6",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cutoff_test2 = ...\n",
    "\n",
    "y_prac_train = ...\n",
    "y_prac_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e379b2-459a-4242-b523-6fb95d9b4ed6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1841c-6554-403b-8956-609d3fd2c7a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Baseline\n",
    "First, let's calculate the baseline MAE for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b9a53-4c0f-4f44-9c35-bee2545eaf2a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_mean = y_train.mean()\n",
    "y_pred_baseline = [y_train_mean] * len(y_train)\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "\n",
    "print(\"Mean P2 Reading:\", round(y_train_mean, 2))\n",
    "print(\"Baseline MAE:\", round(mae_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650281bd-2c4f-4a2c-9dae-2128856c77fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the baseline mean and MAE for the `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd24dc-3857-4ecc-a0e1-736a20067636",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_mean = ...\n",
    "y_prac_pred_baseline = ...\n",
    "mae_baseline_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd19bb-5441-4480-9564-e050ee284605",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Iterating\n",
    "\n",
    "Before we can go any further, we need to instantiate an **autoregression model** based on our `y` training data. We'll call the model `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834de4b-33de-4907-8455-187b21274e20",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model = AutoReg(y_train, lags=24, old_names=False).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492ef77-ca63-44d5-a5c7-5a06ca517f14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice that, unlike our linear regression model which we built using scikit-learn, we're combining instantiation and fitting into one step; statsmodels includes that ability in its `AutoReg` method.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create and fit an autoregression model called `model_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2306ddf-8fc0-49a6-9ab4-91217ff3d2af",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a782b1-78a9-4cb8-9ea9-07f1a3c15d68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Autoregression models need us to generate **in-sample predictions** in order to calculate the MAE of our training data. In-sample predictions use data that's already part of our sample. That's to distinguish it from out-of-sample predictions, which we'll talk about a bit later. The statsmodels library includes a method called [`predict`](https://www.statsmodels.org/stable/examples/notebooks/generated/predict.html) that can help us here. Above, the `AutoReg` method includes this line:\n",
    "\n",
    "```python\n",
    "old_names=False\n",
    "```\n",
    "\n",
    "The `False` value here tells the model that it can use in-sample lagged values to make predictions; if the value had been `True`, the model would have to look elsewhere to make its predictions.\n",
    "\n",
    "Here's how to generate in-sample predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ef5b2-b174-40c5-980e-4b5fc531f420",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74517d7a-1107-43c5-8b32-f2e998c72ae3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once we've done that, we can calculate the MAE of the predictions in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a45dc7-9d76-4c7b-a8c3-7f9ce3e56341",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "training_mae = mean_absolute_error(y_train.loc[y_pred.index], y_pred)\n",
    "print(\"Training MAE:\", training_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576a0b-1c67-4676-b845-3b793208a56d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Generate in-sample predictions using `y_prac`, and find the MAE for your `y_prac` training data. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353abd51-2c1b-4b7b-a79a-7254895c52d9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_pred = ...\n",
    "training_mae_prac = mean_absolute_error(\n",
    "    y_prac_train.loc[y_prac_pred.index], y_prac_pred\n",
    ")  # REMOVERHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee7cbd-374d-486b-a9b9-f403782a897f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Residuals\n",
    "\n",
    "We're going to use our model's residuals to make some visualizations, but first, we need to calculate what those residuals are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40112cb-3de8-45d8-a5cc-5a877cccaa51",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_resid = y_train - y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b73da7-9aa7-4e90-be6c-fa1261979559",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we can make a line plot of our model's residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25be9a-6eeb-4a4d-b989-b2302f59352a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "y_train_resid.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec1560-7153-4f88-810f-3798c25085c6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The ideal residual plot has a random set of datapoints spread evenly on both sides of the line. The plot we just made actually looks pretty good; there are some significant outliers, but, on the whole, the bars describe an even band of values, which is what we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01a41b-b38c-490f-a30a-22b884ac6735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the residuals for `y_prac` and visualize them on a line plot called `fig2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe766c4c-8a19-4f0e-bc1a-9bd5f4d2be76",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_resid = ...\n",
    "fig2, ax = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace2935-2a92-4c58-baab-838c387d08d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's also take a look at a histogram of the residuals to help us see how they're distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73dc0b-79ed-4f86-bbe4-cba01e4d8e3d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_resid.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c74dc-f8a9-4bc7-92c8-89ea4a660f0d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Remember, when we make histograms, we're trying to answer two questions: \n",
    "\n",
    "1.) Is it a normal distribution?\n",
    "2.) Are there any outliers?\n",
    "\n",
    "For our histogram, that middle bar is pretty tall, but the shape described by all the bars looks like a normal distribution (albeit a stretched one), so the answer to the first question is \"yes.\" Outliers are values that fall beyond the shape of a normal distribution, and it doesn't look like we have any of those, so the answer to the second question is \"no.\" Those are the answers we're looking for, so let's move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc893f-77cd-45e4-a298-542acf86a037",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### ACF Plots\n",
    "\n",
    "We're going to make an ACF plot to see how much variation there is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a2534-69aa-465f-b467-92d25e8f7658",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y_train_resid.dropna(), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a078d-a53c-427f-81ac-378f61205fbd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "At first, this might seem wrong, but we're actually looking for a mostly-flat graph here. This is an indication that our model describes all the **seasonality**, or regular changes, in our data. In other words, this graph is exactly what we're looking for.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the make a histogram and an ACF plot of the `y_prac` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438cefc-7f92-4a23-a413-2db4914afb31",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad2b01-0555-43db-bef8-b57cdca6c647",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6aac61-2ddc-4290-8a4f-4c63262cb15f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that we've built an autoregression model that seems to be working pretty well, it's time to **evaluate** it. We've already established that the model works well when compared to itself, but what about how well it works when we start looking outside our original dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5140a44-1a79-48a6-bd50-3689ca7a8844",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Out-of-Sample Predictions\n",
    "\n",
    "To look outside the data, we need to create a new set of predictions. The process here is very similar to the way we made our **baseline** predictions. We're still using [`predict`](https://www.statsmodels.org/0.6.1/examples/notebooks/generated/predict.html), but we're using the `test` data instead of the `train` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eee6fc-e7a9-4dbe-aaa9-4796f2f0001c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(y_test.index.min(), y_test.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0235-b736-4698-8955-4422da9e1174",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have a prediction, we can calculate the MAE of our out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd4fec-efc8-4623-bc5a-f1aa13a0dbf8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"Test MAE 1:\", test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e4e3f-3307-4808-b713-8e100cc3d8a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Generate out-of-sample predictions using your `y_prac` data and `model_prac`, calculate the MAE, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffb661-6b1e-480c-95c6-15652ab3141c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_pred_test = model_prac.predict(\n",
    "    y_prac_test.index.min(), y_prac_test.index.max()\n",
    ")  # REMOVERHS\n",
    "test_mae_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b8196-2997-4ba4-9b7d-c6a13e3f344e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have some out-of-sample predictions, we can compare it to our in-sample predictions using a line plot. The first step there is to create a new DataFrame called `test1_predictions` with two columns: one for the `y_test` data (the true data) and one for the `y_pred` (the predicted data). It's always a good idea to print the first five rows of a new DataFrame, just to make sure it looks the way it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c9f63-2367-4358-b31b-e3022ad9cab5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_predictions = pd.DataFrame(\n",
    "    {\"y_test\": y_test, \"y_pred\": y_pred_test}, index=y_test.index\n",
    ")\n",
    "test1_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc39aa-ee9e-49dc-bffb-85befab48410",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks correct, so we can move on to our line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873f94-a40b-4967-9b5f-cc2df83712ed",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig = px.line(test1_predictions, labels={\"value\": \"P2\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43a10d-4294-4eda-b18a-2107849fa01b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This looks kind of strange, but it's actually exactly what we would expect to see. At the beginning, the `y_pred` data has a fair amount of predictive power, but, as time goes on, the predictions become less and less accurate. It's kind of like what happened with our ACF plots, only in reverse. Last time, the model lost its predictive power as the lag increased. Here, the model loses its predictive power as the horizon — how far away from the present your predictions are — increases.  But don't worry! We'll fix it in a second.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "In the meantime, try it yourself! Make a DataFrame with columns for `y_prac_test` and `y_prac_pred`, and print the result. Then, make a line plot that shows the relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da95516-5715-472c-b1a7-e00ce977c905",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1473d-3599-426c-8ff8-1328273653aa",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6057f23-f543-46b4-9a9a-d27b79f4be16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Walk-forward Validation\n",
    "\n",
    "Our predictions lose power over time because the model gets farther and farther away from its beginning. But what if we could move that beginning forward with the model? That's what **walk-forward validation** is. In a walk-forward validation, we re-train the model at for each new observation in the dataset, dropping the data that's the farthest in the past. Let's say that our prediction for what's going to happen at 12:00 is based on what happened at 11:00, 10:00, and 9:00. When we move forward an hour to predict what's going to happen at 1:00, we only use data from 10:00, 11:00, and 12:00, dropping the data from 9:00 because it's now too far in the past. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156175fb-452d-45ae-b3ae-9145cc2a664d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# First, we define a walk-forward variable\n",
    "y_pred_wfv = pd.Series()\n",
    "# Then, we define a variable that takes into account what's happened in the past\n",
    "history = y_train.copy()\n",
    "# The `for` loop tells the model what to do with those variables.\n",
    "for i in range(len(y_test)):\n",
    "    # Here's where we generate the actual AR model\n",
    "    r = AutoReg(history, 24, old_names=False).fit()\n",
    "    # Now we're using `forecast` to create our next prediction\n",
    "    next_pred = r.forecast()\n",
    "    # We're adding the next prediction to the list\n",
    "    y_pred_wfv = y_pred_wfv.append(next_pred)\n",
    "    # And finally updating `history` to take into account the new observation\n",
    "    history = history.append(y_test[next_pred.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d0d37-4bd5-4366-9e27-c2cbea47ba2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You'll notice that we're using the same `AutoReg` method we used before, only this time, we're using the `y_train` data. Also like before, the `24` is telling the model how many hours it should pay attention to. If you change that number, the MAE will change too.\n",
    "\n",
    "Speaking of the MAE, let's calculate it and see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68baf11-4e5b-4ec1-bdec-c455be2b2e8b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_mae = mean_absolute_error(y_test, y_pred_wfv)\n",
    "print(\"Test MAE 1 (walk forward validation):\", round(test1_mae, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6810ede-f4d3-49eb-afc6-b5214ea806b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Perform a walk-forward validation of your model using the `y_prac_train` data. Then, calculate the MAE and print the result. Note that because we're using `%%capture` in the validation cell, you'll need to create a new cell for your MAE calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aabed-166a-4bc4-a231-926750b5e50e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "y_prac_pred_wfv = ...\n",
    "history_prac = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a364478-628d-4eb4-802e-81ddd01b7dce",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test2_mae = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8dd3c-0960-4a65-abf1-45e8504fec35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Communicating the Results\n",
    "\n",
    "In machine learning, the model's **parameters** are the parts of the model that are **learned** from the training data. There are also **hyperparameters**, which we'll discuss in the next module. For now, just know that parameters come from inside the model, and hyperparameters are specified outside the model.\n",
    "\n",
    "So, let's print the parameters of our validated model and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e58cfb-3d93-4f64-8128-ff589ef80099",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45968a-eee1-4a7d-a620-e2abbe638b80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks pretty good, but showing it in a line plot would be much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51347ee2-ebf5-4700-ab04-2a230dcee19c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_predictions = pd.DataFrame(\n",
    "    {\"y_test\": y_test, \"y_pred\": y_pred_wfv}, index=y_test.index\n",
    ")\n",
    "fig = px.line(test1_predictions)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d6329-6b85-4291-8d61-97225f32da1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks much better! Now our predictions are actually tracking the `test` data, just like they did in the linear regression model.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Access the parameters of `model_prac`, put `y_prac_test` and `y_prac_pred_wfv` into the `test2_predictions` DataFrame, and create a line plot using plotly express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03718244-afbc-4fbc-b5b4-d8039664ddbb",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b46051-9b15-4ea0-9ab9-f061e5419188",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bdc0b-93f3-42eb-884e-353df89bd2e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# ARMA Models & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59166a77-d626-4116-bc20-5edfdc00e1d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**ARMA** stands for Auto Regressive Moving Average, and it's a special kind of **time-series** analysis. So far, we've used autoregression (AR) to build our time-series models, and you might recall that AR models rely on values that remain relatively stable over time. That is, they can predict the future very well, as long as the future looks roughly the same as the past. The trouble with predicting the future is that things can suddenly change, and as a result, the future doesn't look much like the past anymore. These sudden changes — economists call them *endogenous shocks* — can be as big as a hurricane destroying a city or an unexpected increase in the minimum wage, and they can be as small as a new restaurant opening in a neighborhood or a single person losing their job. In our data, the air quality might be changed if there was a nearby forest fire, or if a building collapsed near one of the sensors and raised a giant cloud of dust. \n",
    "\n",
    "Regardless of the size of the shock, ARMA models can *still* predict the future. All we need to make that work is data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ada381-6590-431d-805c-e4eab38903c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "As always, we need to import all the tools we'll need to make our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927ff0e-2c3f-40ab-90ea-3bd1a5469aa0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab299f8-1443-49d7-b91c-2f6a627fe739",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And then we need to get our database client up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dd333-9284-4562-b709-51792117994f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = client[\"air-quality\"]\n",
    "lagos = db[\"lagos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e34a36-c577-4da7-a25c-4de7282e5550",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then, we need to clean our data. All the examples will use data from Site 3; all the practice sets will use Site 2. If you need a refresher on how all those methods work, refer back to the Autoregression notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25412a-7b45-426f-9654-79fc1c4edc00",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "results = lagos.find(\n",
    "    # Note that the `3` refers to Site 3.\n",
    "    {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "    projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    ")\n",
    "df = pd.DataFrame(list(results)).set_index(\"timestamp\")\n",
    "df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")\n",
    "df = df[df[\"P2\"] < 500]\n",
    "df[\"P2.L1\"] = df[\"P2\"].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "y = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d7ee4-1177-4780-84f7-748a52edcc3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Get your client up and running and call your database `db_prac`. Create a variable called `results_prac`, and read in a collection called `lagos_prac` using data from Site 2. Save it as a Series called `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9101327-d4b1-436f-8bdd-d4e33540251a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "db_prac = ...\n",
    "lagos_prac = ...\n",
    "\n",
    "df_prac = ...\n",
    "df_prac.index = ...\n",
    "df_prac = ...\n",
    "df_prac[\"P2.L1\"] = ...\n",
    "\n",
    "y_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8d7f3-7e1a-45d5-a485-028d6cbc10e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Just like we did with AR, we'll start by exploring the data. Let's make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a653f-8739-46f3-a4ae-4f99ef3a6c0d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759698d-4541-48ac-ba69-c7c346810af1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a histogram using `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bb30b-f170-4d1b-8306-086c04178007",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf637f6-fcc6-49f7-88a3-00b5e3805a8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This is what the data looks like when our sample is 1-hour intervals, but we might want to be able to quickly change our sample to other intervals of time. First, we'll create a function called `wrangle`, and then add an **argument**. In Python, arguments tell the function what to do. This function already has an argument called `collection`, so we'll need to add another to make resampling work. We'll call that argument `resamp_pd`. <span style='color: transparent; font-size:1%'>WQU WorldQuant University Applied Data Science Lab QQQQ</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5eea4-c9a1-402a-829f-831f5a6ef6eb",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here's where the new argument goes. We're setting the default value to `\"1H\"`.\n",
    "def wrangle(lagos, resamp_pd=\"1H\"):\n",
    "    results = lagos.find(\n",
    "        # Note that the `3` refers to Site 3.\n",
    "        {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "        projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    "    )\n",
    "    df = pd.DataFrame(list(results)).set_index(\"timestamp\")\n",
    "    df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")\n",
    "    df[\"P2.L1\"] = df[\"P2\"].shift(1)\n",
    "    df.dropna(inplace=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7afae9-51fc-4e6c-bf13-1e012ee86aea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now let's change `\"1H\"` to `\"1D\"` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cb048-127d-4d31-a1ad-a3f0d0e879f0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y = wrangle(lagos, resamp_pd=\"1D\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a1e02-da94-48d0-b595-0ff1dd9e1798",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see on the left side of the table, the samples are now at one day intervals, which is exactly what we wanted!\n",
    "\n",
    "Let's make a new histogram to see if changing the sampling interval made a difference in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b1b22-6d1c-41ca-99bc-51c4788dac4d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f00a1-f1e9-44d9-85df-877c23e94735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This looks pretty different! It's always nice to have a diversified dataset.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Define a function called `wrangle_prac` run it, and print the results of `y_prac`. Then, create a new histogram from `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e5fc1-2922-4fc9-b631-0b33099860f2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(y_prac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefe49c-78e2-418a-af8f-448e54c2e23d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "110e580b-a712-4825-86d2-867558cd7a65",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Like with our AR model, we need to create ACF and PACF plots to see what's happening with the correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b891e1-1400-4ef0-9a67-098120269736",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4810ca-660a-4fee-825a-4487172e6b40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And now let's make a PACF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd30853-5ae9-4bc4-9f2a-bb8d8e830eb7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "fig = plot_pacf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b6bad-4b69-48bb-ad54-c0ea49de4430",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a PAC and a PACF plot using your `y_prac` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78e971-57a2-41ce-b887-c7abfb8b9b48",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190be9b-7173-434c-acb1-5e5f0023f130",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b45224-ec64-4b42-984c-68f99388c531",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Splitting the Data\n",
    "\n",
    "In our AR model, we split our data based on the number of observations we wanted to investigate. This time, we're going to split our data based on the date, using just the readings from October 2018. So, just like we did before, we'll create a training set using `y`, but instead of using percentages to split the data, we'll use dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303daec-2e66-4bc3-95fa-d74fa78dd425",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Notice that the date format is `YYYY-MM-DD`\n",
    "y_train = y.loc[\"2018-10-01\":\"2018-10-31\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea2100-5be5-49c3-b9a0-b1280a05e2eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a training dataset called `y_prac_train` based on November 2017. (Hint: there are 30 days in November.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868ff74-0bab-4664-8269-6484c91daa83",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc82f22-d8fb-473d-be31-0f3ca935a9ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6376f-12a3-4ec0-984a-1a8943e272c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Baseline\n",
    "\n",
    "The first thing we need to do is calculate the MAE for our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bf9b2-05e3-401c-bfc0-08781a402c65",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_mean = y_train.mean()\n",
    "y_pred_baseline = [y_train_mean] * len(y_train)\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "print(\"Mean P2 Reading:\", round(y_train_mean, 2))\n",
    "print(\"Baseline MAE:\", round(mae_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271876c6-222c-4e99-9d08-35adb9ce0fe7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the mean and MAE for the `y_prac` Series, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c6220-b23d-4c57-9be8-0f6afb48bb85",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_mean = ...\n",
    "y_prac_pred_baseline = ...\n",
    "mae_baseline_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7164ecf-d67b-4323-9f5b-1f4f691d89e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Iterating\n",
    "\n",
    "So far, the only difference between our old AR model and the new ARMA model we're building is that the new model's data is based on the date rather than on the length of the variable. But the difference between AR and ARMA is the addition of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceedc11-63e1-429a-b153-58f5092c0cc3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Let's set our `p` values to include values from 0 to 25, moving in steps of 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d940da-21f9-443d-8814-042889639116",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "p_params = range(0, 25, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345f7ce-5939-4162-a995-250e585f6a67",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And let's set our `q` values to include values from 0 to 3, moving in steps of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879cb13-b92c-4409-8e6a-579568716bc0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "q_params = range(0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffaddaf-a3e9-4e6a-99b3-94cb117acf30",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Using `p_params_prac`, set the `p` value to include vales from 1 to 4, moving in steps of 1. Then, using `q_params_prac`, set the `q` value to include values from 0 to 3, moving in steps of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe13001-e730-4677-8061-7875d8e2f31b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "p_params_prac = ...\n",
    "q_params_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bbd2f-1553-46bb-9ec6-42142cbb18dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to tell the model to keep going through all the possible combinations, we'll add in a pair of `for` loops. (If you need a refresher on `for` loops, refer to Notebook 001.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b37a6-eb5e-4a09-808a-4b6ecb321886",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "maes = dict()\n",
    "for p in p_params:\n",
    "    maes[p] = list()\n",
    "    for q in q_params:\n",
    "        order = (p, 0, q)\n",
    "        start_time = time.time()\n",
    "        # Here's where we actually define the model\n",
    "        model = ARIMA(y_train, order=order).fit()\n",
    "        # Here's where we tell the model how we want it to deal with time\n",
    "        elapsed_time = round(time.time() - start_time, 2)\n",
    "        print(f\"Trained ARIMA {order} in {elapsed_time} seconds\")\n",
    "        # Here's where we get back into the MAE for the model\n",
    "        y_pred = model.predict()\n",
    "        mae = mean_absolute_error(y_train.iloc[24:], y_pred.iloc[24:])\n",
    "        # And finally we append the MAES to the original list\n",
    "        maes[p].append(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccee37-7a04-408c-a31b-0e54916a5642",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create an ARMA model called `mode_prac2` based on a dictionary called `maes_prac`, using your training and test data, then print the results and append the MAE to `maes_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eac7b-f22e-4fc4-a32e-9238fdeba9f4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d3aba4-a18f-436c-9958-3e2847a365e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have a working ARMA model, let's turn the output into a DataFrame so we can see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e677fa-4bbe-4ecd-b49f-10efa33a823c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mae_grid = pd.DataFrame(maes)\n",
    "mae_grid.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e47e02-26e2-4cf9-be77-db977b61dfab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And let's visualize the DataFrame in a heatmap. (If you need a refresher on how to create a heatmap in seaborn, refer to Notebook 008.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c6b62-af13-4ae0-9200-ade359605813",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(mae_grid, cmap=\"Blues\")\n",
    "plt.xlabel(\"p values\")\n",
    "plt.ylabel(\"q values\")\n",
    "plt.title(\"Grid Search (Criterion: MAE)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb54cf-ec6e-45b1-942c-5fe040518732",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Turn read the output of your ARMA model into a DataFrame called `mae_grid_prac`, and visualize it in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48084-10ee-4882-ac68-638f6cfda199",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mae_grid_prac = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcf007-0b33-4b49-980d-1a7087590d01",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "It looks like our MAE values are in the right place, but let's try some other ways to explore our new model using the `plot_diagnostics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cec8f-44c0-4287-8fe8-811690846240",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "model.plot_diagnostics(fig=fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d1732-846d-4c99-b215-424db98368b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As usual, we have quite a lot to sift through here. The first graph is showing us our model's residuals. Ideally, we'd like to see this be as close to zero as possible, and this graph is telling us that, for the most part, we have a good model.\n",
    "\n",
    "The next graph over shows us another version of the same thing. The histogram is similar to the one we made before, but there's a pair of lines superimposed. These lines are indicating the **kernel density**, which is another way of saying that it's a smoothed-out version of the blue histogram bars. The green line represents a normal distribution, and is included here just to give you something to compare to the values from your model. The orange line represents the smoothed-out version of the result off your model. Our model is actually pretty close to a normal distribution, so that's good!\n",
    "\n",
    "The Q-Q plot on the bottom left is yet another way to visualize the same thing. Here, the red line is showing us a perfect 1:1 correlation between our variables, and the wavy blue line is showing us what we actually have. Again, our model is pretty close to the red line, so it's looking good.\n",
    "\n",
    "And finally, we have a correlogram, which might look familiar; it's the same kind of plot as the ACF and PACF plots from our AR models. \n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Use `plot_diagnostics` to examine the residuals from `model_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e0901-5ce7-4b16-9836-1d5c0ddd02af",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e52359-095b-4226-bcb7-da25290c3476",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Communicating the Results\n",
    "\n",
    "Now that we have an ARMA model that seems to be working well, it's time to communicate the results of our analysis in a line graph. Let's create a graph that shows the relationship between our training and predicting data. (For a refresher on how to do this and what it means, refer to the AR notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0f2ee-a3a8-4b07-a93b-6f4c729ffa70",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_pred = model.predict()\n",
    "df_predictions = pd.DataFrame(\n",
    "    {\"y_train\": y_train, \"y_pred\": y_train_pred}, index=y_train.index\n",
    ")\n",
    "fig = px.line(df_predictions, labels={\"value\": \"P2\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5931c2-5a9d-4fc1-9ad3-73d7b48d73eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a line plot that compares the `y_prac_train` and `y_prac_pred` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb43227-e5ce-43a0-b414-6b6418a8d63e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74fe2ae3-b552-4790-98a8-b25e9b41c6d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# ARCH and GARCH Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3610f-2e73-4658-9d64-15c1a6e68148",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**ARCH** stands for autoregressive conditionally heteroscedastic, which models the variance of a time series. **ARCH** model assumes variance at time $t$ depends on the **past squared observations**. A **GARCH** (generalized autoregressive conditionally heteroscedastic) model uses values of the **past squared observations** and **past variances** to model the variance at time $t$. **ARCH** and **GARCH** models are widely used in finance and econometric time series analysis. For more details, check out these YouTube videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea5e4c-6146-431a-8ca3-3f7b48b5bd6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"Li95a2biFCU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6453c58-f842-4aba-82af-232d59c6ed8b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"inoBpq1UEn4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba22676-73e7-4710-a905-5b07b69f5772",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's see an example of using **GARCH** model in forecasting Apple stock price volatility. We first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42312b-a221-4f70-986d-40c5bafbeae0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/AAPL.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753bab6-0844-47e0-b98c-7a8614f051a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The dataset range from 1999 to 2022. For simplicity, we only take 5 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1dc72-9f06-4fdc-a7e6-af154b477424",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df = df[\"2015-10-13\":]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af27e-f422-48bc-a3d4-b9b59436e28a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Calculating Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf860e-ef5e-4cbf-889e-2ef0efb523ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The next step is to calculate the volatility of the stock close prices. Here we can use the `pct_change` function from pandas to calculate the daily percentage change, then multiply the result by 100 to get the returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c521c0-9102-4df3-980c-d7afa6e5cd82",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df.sort_index(ascending=True, inplace=True)\n",
    "df[\"return\"] = df[\"close\"].pct_change() * 100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e3213-6a96-4d75-bf22-7e3e72a6b481",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can then take out the return column as our training data. Note the first observation is missing since there is no past value to observe. We need to drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802d2f2-629b-4d83-8939-8f654a01088e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "AAPL_return = df[\"return\"].dropna()\n",
    "AAPL_return.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc224d8-7240-49d2-bc20-fbd3b433f4c1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can check the histogram to see the distribution of the returns over the past five years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3109387-3c18-44a3-811d-006329145341",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "plt.hist(AAPL_return, bins=50)\n",
    "\n",
    "plt.xlabel(\"Returns\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of AAPL Daily Returns\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97473391-ba18-46f6-bc0c-622d8644f83b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "There's a negative outlier in this date range, with the `idxmin` function, we find out it was in 31 August 2020. The stock price has a huge drop due to a [stock split](https://www.cnbc.com/2020/08/31/history-of-apple-stock-splits-says-dont-rush-in-to-buy-cheaper-shares.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e30b67-2687-43a7-ae3e-c4f12d951d1e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "AAPL_return.idxmin(), AAPL_return.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bff06-69e8-4836-be6f-44d1a5b39680",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can also check the standard deviation of the whole dataset with the pandas `std()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d5a12-b376-4d08-98a2-d718a6f143d7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "AAPL_return.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa5e1f-6997-44e7-a1f2-741aa2987db2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To see the statistic more clearly, we can use some time series plots. In addition to the Apple stock price return plot, we can also plot rolling volatility of the return to smooth out the noise, and see how return and volatility are associated with each other. First we can calculate a 50-day rolling standard deviation series for Apple stock price return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e7be6-29e5-4ef0-924d-e7200252aaf0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "AAPL_return_rolling_50d_volatility = AAPL_return.rolling(window=50).std().dropna()\n",
    "AAPL_return_rolling_50d_volatility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf6772-2df9-4d7c-af7a-fe1b0fc7f001",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can plot these two series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5eb08d-e9f2-4945-8e0e-407f708fc376",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "AAPL_return.plot(ax=ax, label=\"daily return\")\n",
    "\n",
    "# Plot\n",
    "AAPL_return_rolling_50d_volatility.plot(\n",
    "    ax=ax, label=\"50d rolling volatility\", linewidth=3\n",
    ")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424031fb-f2b9-4d9e-ae83-e32a615008df",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Here we can see that volatility goes up when the returns change drastically up or down. For instance, when return dropped enormously in 2020 August, volatility also increased a lot. However, this plot reveals a problem. We want to use returns to see if high volatility on one day is associated with high volatility on the following day. But high volatility is caused by large changes in returns, which can be either positive or negative. How can we assess negative and positive numbers together without them canceling each other out? The common solution used in building ARCH or GARCH models are to square the returns. Let's plot the squared returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8b0cb-185f-4700-877b-63a471bc1910",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot squared returns\n",
    "(AAPL_return**2).plot(ax=ax, label=\"daily return\")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"Squared Returns\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52cadd-1c1a-4460-9872-56e58afda94f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    " Now we it much easier to see groups high and low volatility and build a GARCH model. To build a GARCH model, we need to figure out both the `p` and `q` parameters. The `p` parameter is handling correlations at prior time steps and the `q` parameter deals with prior variances, like shocks. It also uses the notion of lag. To see how many lags we should have in our model, we should create an ACF and PACF plot — but using the squared returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4a6eb-6e85-4aed-a963-84bfe6674569",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared returns\n",
    "plot_acf(AAPL_return**2, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f80e6c-cd1a-40b1-b4fb-a0b86ef9f9da",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create PACF of squared returns\n",
    "plot_pacf(AAPL_return**2, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc61d6-641a-4482-bbcf-544e7978a15f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Both the ACF and PACF graph show one lag is enough to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3169684-bdc3-4eed-b964-dfcbc282c31b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3339ba7-b22c-4717-830b-a4f313804dce",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = arch_model(AAPL_return, p=1, q=1, rescale=False).fit(disp=0)\n",
    "\n",
    "print(\"model type:\", type(model))\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ffb59-90f5-4fd6-9c07-3f62fc509dac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Common Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a629c-6bb5-4efd-855e-519b59acf91e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The model summary provides a lot of information about the model, like the trained parameters, model performance, etc. **AIC** and **BIC** are important measurements for model performance. **AIC** stands for Akaike Information Criterion, it measures the goodness of fit of any estimated statistical model. **BIC** is Bayesian Information Criteria that selects model from a finite set of models. When fitting models, it is possible to increase model performance by adding more parameters, which would also result in overfitting. The BIC resolves this problem by introducing a penalty term for the number of parameters in the model. The penalty term is larger in BIC than in AIC. Though BIC is always higher than AIC, lower the value of these two measures, better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf49c80-0477-4477-9690-edd383c7f530",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Standardized Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d240fa-4d3c-4337-92ff-a03784d21be4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After fitting the model with the data, we can get also get the **residuals** to see how the model performs. The residual at time $t$ is the observed return at time $t$ minus the model's estimated mean return at time $t$. For other models that model observations directly, like stock prices, the assumptions are the residuals are the noises that follow a normal distribution. In GARCH model, since we are model returns, which is the variance of the stock prices, the residuals of the variance will not follow a normal distribution. So we need to use **standardized residuals** instead of residuals to check the normality. Standardized residual at time $t$ is the residual at time $t$ divided by the square root of the model estimated variance at time $t$. Let's check the plot of the standardized residuals across the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ea99e-bf61-4c78-ae80-5afaa47234c9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot standardized residuals\n",
    "model.std_resid.plot(ax=ax, label=\"Standardized Residuals\")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05be37b-d356-4f5e-9a87-98ef68edc3ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The standardized residuals are moving around 0 except for the outlier events from 2020 August. Let's check the normality by the histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06705fe-7cb2-4729-8f43-e23e8918223d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Create histogram of standardized residuals\n",
    "plt.hist(model.std_resid, bins=50)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Standardized Residual\")\n",
    "plt.ylabel(\"Frequency [count]\")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Standardized Residuals\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd3cea-a043-4e89-b1d5-1aaee4c177bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If we exclude the outlier, the other standardized residuals do follow a normal distribution with mean 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3e0c6-480e-4dc4-854e-dd8c8c88f3e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We can further evaluate the model by comparing its forecast with a subset of the observed returns to see whether the model has successfully captured the volatility. We can first check the model conditional volatility in its confidence interval throughout the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ad0f7-c3c5-45ae-90c2-42a59b0892a3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot `AAPL_return`\n",
    "AAPL_return.plot(ax=ax)\n",
    "\n",
    "# Plot conditional volatility * 2\n",
    "(2 * model.conditional_volatility).plot(\n",
    "    ax=ax, color=\"C1\", label=\"2 SD Conditional Volatility\"\n",
    ")\n",
    "\n",
    "\n",
    "# Plot conditional volatility * -2\n",
    "(-2 * model.conditional_volatility.rename(\"\")).plot(ax=ax, color=\"C1\")\n",
    "\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Returns\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4035913-bd2f-4d09-823d-8c6ab93cc688",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can then select the test set of the data and do a walk-forward validation on the GARCH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15c421-c3f8-4555-95ab-6b1b40f4f15e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Create empty list to hold predictions\n",
    "predictions = []\n",
    "\n",
    "# Calculate size of test data (20%)\n",
    "test_size = int(len(AAPL_return) * 0.2)\n",
    "\n",
    "# Walk forward\n",
    "for i in range(test_size):\n",
    "    # Create test data\n",
    "    y_train = AAPL_return.iloc[: -(test_size - i)]\n",
    "\n",
    "    # Train model\n",
    "    model = arch_model(y_train, p=1, q=1, rescale=False).fit(disp=0)\n",
    "\n",
    "    # Generate next prediction (volatility, not variance)\n",
    "\n",
    "    next_pred = model.forecast(horizon=1, reindex=False).variance.values[0][0] ** 0.5\n",
    "\n",
    "    # Append prediction to list\n",
    "    predictions.append(next_pred)\n",
    "\n",
    "# Create Series from predictions list\n",
    "y_test_wfv = pd.Series(predictions, index=AAPL_return.tail(test_size).index)\n",
    "\n",
    "print(\"y_test_wfv type:\", type(y_test_wfv))\n",
    "print(\"y_test_wfv shape:\", y_test_wfv.shape)\n",
    "y_test_wfv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6eb44-1865-4edf-8799-a66e2e527314",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can plot the predicted volatility versus return in this test set, and see the model has captured most of the variance here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2345192-c38c-4280-9888-1b0af9b502dc",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot returns for test data\n",
    "AAPL_return.tail(test_size).plot(ax=ax, label=\"AAPL return\")\n",
    "\n",
    "# Plot volatility predictions * 2\n",
    "(2 * y_test_wfv).plot(ax=ax, c=\"C1\", label=\"2 SD Predicted Volatility\")\n",
    "\n",
    "# Plot volatility predictions * -2\n",
    "(-2 * y_test_wfv).plot(ax=ax, c=\"C1\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4012f64-e48e-4795-92d4-4ac6e14ad8b7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a806c5-0353-4f12-b6c2-73b828a9dc42",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The last step is to make the forecasts with our trained model. Let's make a one-day forecast first to see how GARCH model forecasting works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206391a6-53f9-49dd-9ee6-d8f094b4f4c3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "one_day_forecast = model.forecast(horizon=1, reindex=False).variance\n",
    "\n",
    "print(\"one_day_forecast type:\", type(one_day_forecast))\n",
    "one_day_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bff71-621c-40ba-aae1-e4fbb0f504d8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "There are two things we need to keep in mind here. First, our model forecast shows the predicted variance, not the standard deviation / volatility. So we'll need to take the square root of the value. Second, the prediction's index is in the format of `\"h.1\"` as the next date of the last training data date. It's not in the form of DatetimeIndex that we desire. So we need to format the forecasts to be more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61924970-fda4-4d06-9eba-378779af33b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We first generate 5 days predictions with our trained GARCH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4073de8-819f-4b02-9dd3-00021a66cf27",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Generate 5-day volatility forecast\n",
    "prediction = model.forecast(horizon=5, reindex=False).variance ** 0.5\n",
    "\n",
    "# Calculate forecast start date\n",
    "start = prediction.index[0] + pd.DateOffset(days=1)\n",
    "\n",
    "# Create date range\n",
    "prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "# Create prediction index labels, ISO 8601 format\n",
    "prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "print(\"prediction_index type:\", type(prediction_index))\n",
    "print(\"prediction_index len:\", len(prediction_index))\n",
    "prediction_index[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09df92-e6fc-4af0-a7ea-d7e797c1690b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then we define a function to clean the predictions to be more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eed946-a340-4b00-b196-a8007a853ede",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Take the square root of the model forecasts\n",
    "data = prediction.values.flatten() ** 0.5\n",
    "\n",
    "# Format the forecasts with the correct date\n",
    "prediction_formatted = pd.Series(data, index=prediction_index)\n",
    "\n",
    "# Show the results\n",
    "prediction_formatted.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b64806-b6ea-44d9-9bb8-3072275591ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# References & Further Reading\n",
    "\n",
    "- [More on ARMA models](https://365datascience.com/tutorials/time-series-analysis-tutorials/arma-model/)\n",
    "- [Even more in ARMA models](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "- [Information on p and q parameters](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "- [A primer on autoregression](https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/)\n",
    "- [More information on ACF plots](https://www.statisticshowto.com/correlogram/#:~:text=A%20correlogram%20(also%20called%20Auto,a%20subsequent%20point%20in%20time.)\n",
    "- [A primer on ACF and PACF](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/)\n",
    "- [Background on residuals](https://www.statisticshowto.com/residual/)\n",
    "- [More on walk-forward validation](https://www.tutorialspoint.com/time_series/time_series_walk_forward_validation.htm)\n",
    "- [Reading on parameters and hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32876e0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
